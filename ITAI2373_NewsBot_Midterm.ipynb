{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcLSbYaqKH72cyNwCqjV6z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stillrahim/jupyter-exploration/blob/main/ITAI2373_NewsBot_Midterm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i8rDi4gZp5G"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install -q kaggle pandas numpy sklearn nltk tqdm\n",
        "\n",
        "# Optional for reading jsonlines faster\n",
        "!pip install -q jsonlines\n",
        "\n",
        "# NLTK downloads (for later preprocessing if you want)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload kaggle.json (only if using Kaggle API)\n",
        "from google.colab import files, drive\n",
        "import os, sys\n",
        "\n",
        "print(\"If you plan to use Kaggle API, upload your kaggle.json now. Otherwise, skip this cell.\")\n",
        "uploaded = files.upload()  # choose kaggle.json\n",
        "\n",
        "# Move to .kaggle\n",
        "if 'kaggle.json' in uploaded:\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    with open('/root/.kaggle/kaggle.json','wb') as f:\n",
        "        f.write(uploaded['kaggle.json'])\n",
        "    os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "    print(\"✅ kaggle.json installed.\")\n",
        "else:\n",
        "    print(\"No kaggle.json uploaded — proceed with manual option or upload later.\")\n"
      ],
      "metadata": {
        "id": "8Y3l9FEMZu5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose which dataset to fetch. Options: 'bbc', 'news_category', 'all_the_news', 'manual'\n",
        "# If 'manual', you will upload file(s) in the next cell.\n",
        "DATA_CHOICE = 'bbc'   # <-- change this as needed: 'bbc', 'news_category', 'all_the_news', or 'manual'\n"
      ],
      "metadata": {
        "id": "zibFhSpyZw2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, zipfile, glob, subprocess\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    print(\">>>\", cmd)\n",
        "    r = subprocess.run(cmd, shell=True, check=False, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "    print(r.stdout.decode('utf-8', errors='ignore'))\n",
        "\n",
        "if DATA_CHOICE == 'bbc':\n",
        "    # BBC competition download\n",
        "    print(\"Downloading BBC News (learn-ai-bbc competition). This requires kaggle.json uploaded.\")\n",
        "    run_cmd('kaggle competitions download -c learn-ai-bbc -p /content --force')\n",
        "    # unzip\n",
        "    if os.path.exists('/content/learn-ai-bbc.zip'):\n",
        "        run_cmd('unzip -o /content/learn-ai-bbc.zip -d /content')\n",
        "elif DATA_CHOICE == 'news_category':\n",
        "    print(\"Downloading News Category Dataset (rmisra/news-category-dataset).\")\n",
        "    run_cmd('kaggle datasets download -d rmisra/news-category-dataset -p /content --force')\n",
        "    if os.path.exists('/content/news-category-dataset.zip'):\n",
        "        run_cmd('unzip -o /content/news-category-dataset.zip -d /content')\n",
        "elif DATA_CHOICE == 'all_the_news':\n",
        "    print(\"Downloading All the News dataset (snapcrack/all-the-news).\")\n",
        "    run_cmd('kaggle datasets download -d snapcrack/all-the-news -p /content --force')\n",
        "    if os.path.exists('/content/all-the-news.zip'):\n",
        "        run_cmd('unzip -o /content/all-the-news.zip -d /content')\n",
        "elif DATA_CHOICE == 'manual':\n",
        "    print(\"Manual mode selected. Please upload CSV or JSON files in the next cell.\")\n",
        "else:\n",
        "    raise ValueError(\"Unknown DATA_CHOICE. Set to 'bbc', 'news_category', 'all_the_news', or 'manual'.\")\n"
      ],
      "metadata": {
        "id": "aKJnaTBeZzyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run if you chose DATA_CHOICE = 'manual'\n",
        "from google.colab import files\n",
        "if DATA_CHOICE == 'manual':\n",
        "    print(\"Upload your dataset file(s) now (CSV or JSON/JSONL).\")\n",
        "    uploaded = files.upload()\n",
        "    print(\"Uploaded:\", list(uploaded.keys()))\n",
        "else:\n",
        "    print(\"Skipping manual upload (DATA_CHOICE != 'manual').\")\n"
      ],
      "metadata": {
        "id": "5Iyys22kZ1Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, json, os, jsonlines, glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# search for likely datasets in current dir\n",
        "candidates = glob.glob('/content/**/*.csv', recursive=True) + glob.glob('/content/**/*.json', recursive=True) + glob.glob('/content/**/*.jsonl', recursive=True)\n",
        "candidates = list(set(candidates))\n",
        "print(\"Candidate files found:\")\n",
        "for f in candidates:\n",
        "    print(\" -\", f)\n",
        "\n",
        "# Heuristic loader for common known files\n",
        "def try_load_any():\n",
        "    # Try BBC style: bbc-text.csv or train.csv\n",
        "    bbc_files = [p for p in candidates if os.path.basename(p).lower().startswith('bbc') or os.path.basename(p).lower().startswith('train')]\n",
        "    if bbc_files:\n",
        "        print(\"Trying BBC-style CSV:\", bbc_files[0])\n",
        "        return pd.read_csv(bbc_files[0])\n",
        "    # Try News Category JSONL\n",
        "    news_jsonl = [p for p in candidates if 'news' in os.path.basename(p).lower() and p.lower().endswith('.json')]\n",
        "    if news_jsonl:\n",
        "        # try reading jsonlines\n",
        "        path = news_jsonl[0]\n",
        "        try:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                # determine if newline-delimited JSON\n",
        "                first = f.readline()\n",
        "                if first.strip().startswith('{'):\n",
        "                    # assume jsonlines\n",
        "                    data = []\n",
        "                    with jsonlines.open(path) as reader:\n",
        "                        for obj in reader:\n",
        "                            data.append(obj)\n",
        "                    return pd.DataFrame(data)\n",
        "                else:\n",
        "                    # try normal json\n",
        "                    f.seek(0)\n",
        "                    obj = json.load(f)\n",
        "                    return pd.DataFrame(obj)\n",
        "        except Exception as e:\n",
        "            print(\"Failed json load:\", e)\n",
        "    # pick any CSV\n",
        "    csvs = [p for p in candidates if p.lower().endswith('.csv')]\n",
        "    if csvs:\n",
        "        print(\"Trying generic CSV:\", csvs[0])\n",
        "        return pd.read_csv(csvs[0], encoding='utf-8', error_bad_lines=False)\n",
        "    # pick any jsonl\n",
        "    jsonls = [p for p in candidates if p.lower().endswith('.jsonl') or p.lower().endswith('.json')]\n",
        "    if jsonls:\n",
        "        print(\"Trying generic JSON/JSONL:\", jsonls[0])\n",
        "        try:\n",
        "            with jsonlines.open(jsonls[0]) as reader:\n",
        "                data = [obj for obj in reader]\n",
        "            return pd.DataFrame(data)\n",
        "        except Exception as e:\n",
        "            print(\"Generic JSON read failed:\", e)\n",
        "    raise FileNotFoundError(\"No suitable dataset file found. Please upload CSV/JSON as instructed.\")\n",
        "\n",
        "# load\n",
        "df = try_load_any()\n",
        "print(\"Loaded dataframe with shape:\", df.shape)\n",
        "display(df.head(3))\n"
      ],
      "metadata": {
        "id": "FZZ19UEcZ3Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heuristics for column names\n",
        "possible_text_cols = ['text','content','article','headline','description','body','clean_text']\n",
        "possible_cat_cols  = ['category','label','class','topic','news_desk','section']\n",
        "\n",
        "cols = df.columns.tolist()\n",
        "text_col = None\n",
        "cat_col = None\n",
        "\n",
        "for c in possible_text_cols:\n",
        "    if c in cols:\n",
        "        text_col = c\n",
        "        break\n",
        "for c in possible_cat_cols:\n",
        "    if c in cols:\n",
        "        cat_col = c\n",
        "        break\n",
        "\n",
        "# If still None, try to guess: longest text column\n",
        "if text_col is None:\n",
        "    # choose column with longest average string length\n",
        "    text_candidates = [c for c in cols if df[c].dtype == object]\n",
        "    if text_candidates:\n",
        "        avg_len = {c: df[c].dropna().astype(str).map(len).mean() for c in text_candidates}\n",
        "        text_col = max(avg_len, key=avg_len.get)\n",
        "        print(\"Guessed text column as:\", text_col)\n",
        "    else:\n",
        "        raise ValueError(\"No text-like columns found. Please provide dataset with text content.\")\n",
        "\n",
        "if cat_col is None:\n",
        "    # try to pick a short string/object column with few unique values\n",
        "    obj_cols = [c for c in cols if df[c].dtype == object and c != text_col]\n",
        "    candidate_scores = {}\n",
        "    for c in obj_cols:\n",
        "        nunique = df[c].nunique(dropna=True)\n",
        "        candidate_scores[c] = nunique\n",
        "    if candidate_scores:\n",
        "        cat_col = min(candidate_scores, key=candidate_scores.get)\n",
        "        print(\"Guessed category column as:\", cat_col)\n",
        "    else:\n",
        "        print(\"No obvious category column found — you will need to add category labels before sampling.\")\n",
        "        cat_col = None\n",
        "\n",
        "print(\"Using text column:\", text_col, \"category column:\", cat_col)\n",
        "\n",
        "# Basic cleaning\n",
        "df[text_col] = df[text_col].astype(str).str.strip()\n",
        "if cat_col:\n",
        "    df[cat_col] = df[cat_col].astype(str).str.strip()\n",
        "\n",
        "# Drop missing\n",
        "if cat_col:\n",
        "    df_clean = df.dropna(subset=[text_col, cat_col]).copy()\n",
        "else:\n",
        "    df_clean = df.dropna(subset=[text_col]).copy()\n",
        "\n",
        "print(\"After dropna shape:\", df_clean.shape)\n"
      ],
      "metadata": {
        "id": "GasOFZf7Z6LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "MAX_ROWS = 2000\n",
        "MIN_ROWS = 500\n",
        "MIN_CATEGORIES = 4\n",
        "\n",
        "if cat_col:\n",
        "    # Count categories\n",
        "    cat_counts = df_clean[cat_col].value_counts()\n",
        "    print(\"Category counts (top 10):\")\n",
        "    print(cat_counts.head(10))\n",
        "\n",
        "    num_cats = cat_counts.shape[0]\n",
        "    total_rows = len(df_clean)\n",
        "    print(f\"Total rows: {total_rows}, Categories: {num_cats}\")\n",
        "\n",
        "    # If too many rows, sample stratified by category proportionally\n",
        "    if total_rows > MAX_ROWS:\n",
        "        print(f\"Sampling down to {MAX_ROWS} rows (stratified by category).\")\n",
        "        # Compute proportion per class\n",
        "        proportions = cat_counts / cat_counts.sum()\n",
        "        # desired per class (at least 1)\n",
        "        desired = (proportions * MAX_ROWS).round().astype(int)\n",
        "        # ensure sum matches MAX_ROWS due to rounding\n",
        "        diff = MAX_ROWS - desired.sum()\n",
        "        if diff != 0:\n",
        "            # adjust top classes\n",
        "            idx = desired.sort_values(ascending=False).index.tolist()\n",
        "            i = 0\n",
        "            while diff != 0:\n",
        "                desired[idx[i % len(idx)]] += np.sign(diff)\n",
        "                diff = MAX_ROWS - desired.sum()\n",
        "                i += 1\n",
        "        # perform sampling per class\n",
        "        sampled_frames = []\n",
        "        for cls, n in desired.items():\n",
        "            subset = df_clean[df_clean[cat_col]==cls]\n",
        "            if len(subset) <= n:\n",
        "                sampled_frames.append(subset)\n",
        "            else:\n",
        "                sampled_frames.append(subset.sample(n=n, random_state=42))\n",
        "        df_sampled = pd.concat(sampled_frames).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    else:\n",
        "        df_sampled = df_clean.copy()\n",
        "\n",
        "    # Post-sampling checks\n",
        "    final_rows = len(df_sampled)\n",
        "    final_cats = df_sampled[cat_col].nunique()\n",
        "    print(f\"Final dataset rows: {final_rows}, categories: {final_cats}\")\n",
        "\n",
        "else:\n",
        "    # No categories identified — just sample up to MAX_ROWS\n",
        "    print(\"No category column detected; sampling up to MAX_ROWS rows.\")\n",
        "    if len(df_clean) > MAX_ROWS:\n",
        "        df_sampled = df_clean.sample(n=MAX_ROWS, random_state=42)\n",
        "    else:\n",
        "        df_sampled = df_clean.copy()\n",
        "\n",
        "# If we still have more than MAX_ROWS (safety)\n",
        "if len(df_sampled) > MAX_ROWS:\n",
        "    df_sampled = df_sampled.sample(n=MAX_ROWS, random_state=42)\n",
        "\n",
        "# Final sanity\n",
        "print(\"Sampled dataset shape:\", df_sampled.shape)\n",
        "display(df_sampled.head(2))\n"
      ],
      "metadata": {
        "id": "-MorYkUGZ7WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate required constraints\n",
        "ok = True\n",
        "msgs = []\n",
        "\n",
        "if len(df_sampled) < MIN_ROWS:\n",
        "    ok = False\n",
        "    msgs.append(f\"Dataset has only {len(df_sampled)} rows (< {MIN_ROWS}). Consider adding more articles.\")\n",
        "\n",
        "if cat_col:\n",
        "    unique_cats = df_sampled[cat_col].nunique()\n",
        "    if unique_cats < MIN_CATEGORIES:\n",
        "        ok = False\n",
        "        msgs.append(f\"Only {unique_cats} categories found (< {MIN_CATEGORIES}). Consider using a dataset with more categories or remapping labels.\")\n",
        "\n",
        "print(\"Validation results:\")\n",
        "if ok:\n",
        "    print(\"✅ Dataset meets minimum requirements.\")\n",
        "else:\n",
        "    print(\"⚠️ Dataset does NOT meet requirements:\")\n",
        "    for m in msgs:\n",
        "        print(\" -\", m)\n",
        "\n",
        "# rename columns to standard names expected downstream\n",
        "if cat_col:\n",
        "    df_final = df_sampled.rename(columns={text_col: 'content', cat_col: 'category'})[['content','category']]\n",
        "else:\n",
        "    df_final = df_sampled.rename(columns={text_col: 'content'})[['content']]\n",
        "\n",
        "# Save locally and to Drive (optional)\n",
        "outname = 'newsbot_dataset.csv'\n",
        "df_final.to_csv(outname, index=False)\n",
        "print(f\"Saved prepared dataset to /content/{outname}\")\n",
        "\n",
        "# Optionally mount Drive and save there too\n",
        "save_to_drive = False  # change to True if you want to save to Drive\n",
        "if save_to_drive:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    drive_path = '/content/drive/MyDrive/' + outname\n",
        "    df_final.to_csv(drive_path, index=False)\n",
        "    print(\"Also saved to Google Drive:\", drive_path)\n"
      ],
      "metadata": {
        "id": "4iyWAe4YZ9Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate required constraints\n",
        "ok = True\n",
        "msgs = []\n",
        "\n",
        "if len(df_sampled) < MIN_ROWS:\n",
        "    ok = False\n",
        "    msgs.append(f\"Dataset has only {len(df_sampled)} rows (< {MIN_ROWS}). Consider adding more articles.\")\n",
        "\n",
        "if cat_col:\n",
        "    unique_cats = df_sampled[cat_col].nunique()\n",
        "    if unique_cats < MIN_CATEGORIES:\n",
        "        ok = False\n",
        "        msgs.append(f\"Only {unique_cats} categories found (< {MIN_CATEGORIES}). Consider using a dataset with more categories or remapping labels.\")\n",
        "\n",
        "print(\"Validation results:\")\n",
        "if ok:\n",
        "    print(\"✅ Dataset meets minimum requirements.\")\n",
        "else:\n",
        "    print(\"⚠️ Dataset does NOT meet requirements:\")\n",
        "    for m in msgs:\n",
        "        print(\" -\", m)\n",
        "\n",
        "# rename columns to standard names expected downstream\n",
        "if cat_col:\n",
        "    df_final = df_sampled.rename(columns={text_col: 'content', cat_col: 'category'})[['content','category']]\n",
        "else:\n",
        "    df_final = df_sampled.rename(columns={text_col: 'content'})[['content']]\n",
        "\n",
        "# Save locally and to Drive (optional)\n",
        "outname = 'newsbot_dataset.csv'\n",
        "df_final.to_csv(outname, index=False)\n",
        "print(f\"Saved prepared dataset to /content/{outname}\")\n",
        "\n",
        "# Optionally mount Drive and save there too\n",
        "save_to_drive = False  # change to True if you want to save to Drive\n",
        "if save_to_drive:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    drive_path = '/content/drive/MyDrive/' + outname\n",
        "    df_final.to_csv(drive_path, index=False)\n",
        "    print(\"Also saved to Google Drive:\", drive_path)\n"
      ],
      "metadata": {
        "id": "60pYOgIUZ_Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final dataset preview:\")\n",
        "display(df_final.sample(n=min(5,len(df_final)), random_state=42))\n",
        "\n",
        "if 'category' in df_final.columns:\n",
        "    print(\"\\nCategory distribution:\")\n",
        "    print(df_final['category'].value_counts())\n"
      ],
      "metadata": {
        "id": "rsGj5YL0aA6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}