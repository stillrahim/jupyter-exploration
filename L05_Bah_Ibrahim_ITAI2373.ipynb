{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPthThEAz/2bHIrMyO4A8dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stillrahim/jupyter-exploration/blob/main/L05_Bah_Ibrahim_ITAI2373.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-v0HgR0VXKz"
      },
      "outputs": [],
      "source": [
        "# Lab 05 — Part-of-Speech Tagging in the Real World\n",
        "**Name:** Ibrahim Bah\n",
        "**Course:** ITAI 2373\n",
        "**Notebook:** L05_LastName_FirstName_ITAI2373.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= Setup =======\n",
        "# Colab-friendly installs (runs quickly)\n",
        "!pip install -q nltk spacy pandas matplotlib seaborn\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Imports\n",
        "import nltk\n",
        "import spacy\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import treebank, brown\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tag import map_tag\n",
        "\n",
        "# NLTK data download\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n"
      ],
      "metadata": {
        "id": "ggJCwHx0VcBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sample_sentences = [\n",
        "    \"Apple is releasing a new product next week.\",\n",
        "    \"I will book a flight tomorrow, then read a book in the evening.\",\n",
        "    \"Can you help me reset my password? I'm locked out!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\"\n",
        "]\n",
        "\n",
        "print(\"Sample sentences:\\n\")\n",
        "for s in sample_sentences:\n",
        "    print(\"-\", s)\n"
      ],
      "metadata": {
        "id": "jN5a5GzmVjRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK tagging\n",
        "print(\"\\n--- NLTK tagging (Penn Treebank tags) ---\")\n",
        "for s in sample_sentences:\n",
        "    tokens = word_tokenize(s)\n",
        "    tags = pos_tag(tokens)  # Penn Treebank by default\n",
        "    print(s)\n",
        "    print(tags)\n",
        "    print()\n",
        "\n",
        "# SpaCy tagging\n",
        "print(\"\\n--- SpaCy tagging (tag_ and pos_) ---\")\n",
        "for s in sample_sentences:\n",
        "    doc = nlp(s)\n",
        "    print(s)\n",
        "    print([(token.text, token.tag_, token.pos_) for token in doc])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "8R_EPwNgVqqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Map PTB tags to Universal tagset for fairer comparison\n",
        "NLTK provides `map_tag` to map PTB tags to the Universal tagset. SpaCy also provides `pos_` (Universal-style coarse POS).\n"
      ],
      "metadata": {
        "id": "f69RTUHhVr3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nltk_to_universal(tagged):\n",
        "    # tagged: list of (word, PTB tag)\n",
        "    return [(w, map_tag('en-ptb', 'universal', t)) for w, t in tagged]\n",
        "\n",
        "print(\"NLTK -> Universal mapping examples:\")\n",
        "for s in sample_sentences:\n",
        "    tokens = word_tokenize(s)\n",
        "    tags = pos_tag(tokens)\n",
        "    print(nltk_to_universal(tags))\n"
      ],
      "metadata": {
        "id": "dtuGO9K6VwOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 2 — Handling Messy, Real-World Text\n",
        "We'll demonstrate pre-processing of messy text (social-media-like), then tag it.\n"
      ],
      "metadata": {
        "id": "9oJWSJRLVx5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messy_texts = [\n",
        "    \"OMG!!! I can't login :(( help pls!!! #frustrated\",\n",
        "    \"thx 4 the quick reply. ill try again :)\",\n",
        "    \"Order #1234 not delivered yet!!! wtf?\",\n",
        "    \"Got it—thx. all good ✅\"\n",
        "]\n",
        "\n",
        "def clean_for_tagging(s):\n",
        "    # basic cleaning for demonstration; keep emoticons for analysis\n",
        "    s = s.replace(\"#\", \" #\")\n",
        "    return s\n",
        "\n",
        "for s in messy_texts:\n",
        "    s_clean = clean_for_tagging(s)\n",
        "    print(\"Original:\", s)\n",
        "    print(\"NLTK tokens/tags:\", pos_tag(word_tokenize(s_clean)))\n",
        "    doc = nlp(s_clean)\n",
        "    print(\"SpaCy tokens/tags:\", [(t.text, t.pos_, t.tag_) for t in doc])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "WhCxlpymV01V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Discussion / Observations\n",
        "- Note where tokenizers differ (e.g., emoticons, hashtags, contractions).\n",
        "- Does stopword removal or lowercasing change tagging correctness? (Try it and report.)\n"
      ],
      "metadata": {
        "id": "Rll3QhR0V304"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Part 2 — Customer Service Case Study (toy dataset)\n",
        "We will:\n",
        "1. Create a small set of mock transcripts with labels (urgent/non-urgent),\n",
        "2. POS-tag them,\n",
        "3. Compute POS tag frequency features and visualize.\n"
      ],
      "metadata": {
        "id": "wTBvnMR9V6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy customer service transcripts\n",
        "transcripts = [\n",
        "    {\"id\": 1, \"text\": \"Hi, my internet is down since last night. Please help ASAP!!!\", \"urgent\": 1},\n",
        "    {\"id\": 2, \"text\": \"Hello, I'd like to change my billing address at my convenience.\", \"urgent\": 0},\n",
        "    {\"id\": 3, \"text\": \"I can't access my account and I need it for work today.\", \"urgent\": 1},\n",
        "    {\"id\": 4, \"text\": \"Where can I find your return policy? Thanks!\", \"urgent\": 0},\n",
        "    {\"id\": 5, \"text\": \"Server is down, business critical. Please escalate.\", \"urgent\": 1}\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(transcripts)\n",
        "df\n"
      ],
      "metadata": {
        "id": "DJ8rs23pV-G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tag all transcripts using SpaCy and NLTK\n",
        "def pos_counts_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    counts = Counter([token.pos_ for token in doc])\n",
        "    return counts\n",
        "\n",
        "def pos_counts_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tags = pos_tag(tokens)\n",
        "    univ = [map_tag('en-ptb', 'universal', t) for _, t in tags]\n",
        "    return Counter(univ)\n",
        "\n",
        "df['spacy_pos_counts'] = df['text'].apply(pos_counts_spacy)\n",
        "df['nltk_pos_counts'] = df['text'].apply(pos_counts_nltk)\n",
        "df\n"
      ],
      "metadata": {
        "id": "FPnHda1yWBZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert counters to a DataFrame of frequencies for plotting\n",
        "def counters_to_df(counter_series):\n",
        "    rows = []\n",
        "    for i, c in enumerate(counter_series, start=1):\n",
        "        r = dict(c)\n",
        "        r['id'] = i\n",
        "        rows.append(r)\n",
        "    return pd.DataFrame(rows).set_index('id').fillna(0).astype(int)\n",
        "\n",
        "spacy_counts_df = counters_to_df(df['spacy_pos_counts'])\n",
        "nltk_counts_df = counters_to_df(df['nltk_pos_counts'])\n",
        "\n",
        "print(\"SpaCy POS counts (per transcript):\")\n",
        "display(spacy_counts_df)\n",
        "print(\"NLTK (Universal) POS counts (per transcript):\")\n",
        "display(nltk_counts_df)\n"
      ],
      "metadata": {
        "id": "hPEInV27WGGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: POS tag distribution (SpaCy)\n",
        "plt.figure(figsize=(10,5))\n",
        "spacy_counts_df.sum().sort_values(ascending=False).plot(kind='bar')\n",
        "plt.title(\"Aggregate POS Frequency (SpaCy) — Transcripts\")\n",
        "plt.xlabel(\"POS tag\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uiXIOnZIWH5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot side-by-side comparison (SpaCy vs NLTK universal mapping) for totals\n",
        "total_spacy = spacy_counts_df.sum()\n",
        "total_nltk = nltk_counts_df.sum()\n",
        "\n",
        "cmp_df = pd.DataFrame({'SpaCy': total_spacy, 'NLTK': total_nltk}).fillna(0)\n",
        "cmp_df.plot(kind='bar', figsize=(12,5))\n",
        "plt.title(\"POS Frequency Comparison: SpaCy vs NLTK (Universal)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0BR2zSUnWJ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick feature: compute verb + AUX + ADV counts and compare urgent vs non-urgent\n",
        "def verb_adv_score(counter):\n",
        "    # SpaCy POS tags: VERB, AUX, ADV\n",
        "    return counter.get('VERB',0) + counter.get('AUX',0) + counter.get('ADV',0)\n",
        "\n",
        "df['spacy_verb_adv_score'] = df['spacy_pos_counts'].apply(verb_adv_score)\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x='id', y='spacy_verb_adv_score', hue='urgent', data=df)\n",
        "plt.title(\"Verb/Aux/Adv score per transcript (higher often indicates urgency)\")\n",
        "plt.xlabel(\"Transcript ID\")\n",
        "plt.ylabel(\"Verb/Aux/Adv count\")\n",
        "plt.legend(title='Urgent')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3h_o65bEWWYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}